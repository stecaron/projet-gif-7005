Ici, on parle de la manière dont on applique les grandes lignes décrites auparavant.

Points à traiter:
	- Méthodologie de validation (Séparation du jeu de données)
	- Mesure de score utilisée (Précision sur recommandation de 5 documents car métrique d'évaluation de Coveo)
	- Utilisation des données sans clicks et à plusieurs clicks
	- Pipeline et recherche en grille
	- Paramètres testés
	
	
- Début du vrai texte

Avant même de créer les attributs, il était nécessaire de faire la jonction entre les différents jeux de données pour entres autres indiquer au modèle quelle requête a menée à quel document. D'abord, voici les différents jeux de données (avec le nombre d'observations) à notre disposition:

\begin{center}
  \begin{tabular}{ |c|c|c|c| } 
     \hline
     Type & E & V & T \\
     \hline
     \hline
     \textit{searches} & 52133 & 14895 & 7448 \\ 
     \hline
     \textit{clicks} & 24491 & 6920 & 3567 \\ 
     \hline
  \end{tabular}
\end{center}

La colonne E correspond à l'ensemble d'entraînement, la colonne V à l'ensemble de validation et la colonne T correspond à l'emsemble test. Pour ce dernier, nous avions accès aux recherches seulement et non au clicks. Comme on peut le voir, il y a plus de recherches que de clicks. Cela veut dire que certaines recherches ont mené vers aucun click. Une même recherche peut également avoir menée à plusieurs clicks. En bref, il existe plusieurs façons de joindre ces recherches avec ces clicks. Afin de nous simplifier la vie, nous avons retiré les recherches qui n'ont pas mené vers un click. De plus, nous avons également garder seulement le dernier click, en se basant sur le moment du click, pour une même recherche. Notre raisonnement derrière est que le dernier click est probablement le "bon" et que les précédents ajoutent du bruit au modèle. En somme, voici le nombre d'observations que nous avons pour les ensembles d'entraînement et de validation après ces traitements:

\begin{center}
  \begin{tabular}{ |c|c|c|c| } 
     \hline
     Type & E & V \\
     \hline
     \hline
     \textit{searches/clicks} & 18571 & 6920 \\ 
     \hline
  \end{tabular}
\end{center}

\subsection{Prétaitements des données}

Maintenant que nous avons des observations avec des variables explicatives et des étiquettes, la prochaine étape consiste à effectuer certains prétraitements sur ce jeu de données.

Avant de débuter l'optimisation de notre modèle, nous avons défini quelle mesure de score allait être utilisée pour l'évaluation de notre modèle afin de baser nos développements sur celle-ci.
Puisque l'évaluation de notre modèle sera faire en regardant si le document pertinent se trouve dans la liste des 5 documents les plus pertinents fournis selon notre modèle, nous nous sommes bâti une fonction de score qui retourne le pourcentage de réussite selon ce critère particulier.

Également, afin d'évaluer notre modèle, nous avons décidé d'utiliser le partitionnement des données déjà fait par Coveo. Nous avons vérifié que cette séparation des données avait été faite de façon aléatoire en confirmant que les dates de recherche n'étaient pas ordonnées.

Puisque nos données brutes contiennent des informations de plusieurs types, dont des données textuelles qu'on ne peut pas directement utiliser dans les algorithmes d'apprentissage automatique, nous devions faire non seulement beaucoup de travail sur l'optimisation des hyper-paramètres de nos modèles, mais aussi sur le choix des pré-traitements à faire sur nos données.

Pour attaquer de façon claire le problème de l'optimisation des pré-traitements, nous avons utilisé le module \emph{pipeline} de la librairie Python \emph{sklearn}.
Ce dernier nous permet de définir nos différentes étapes de pré-traitement par des classes dont les paramètres sont modifiables.
On procédant ainsi, on peut simplement faire une recherche en grille comme on le ferait avec n'importe quel modèle, mais en testant plutôt différentes combinaisons de pré-traitements.

\subsection{Modélisation}

Également, puisque le nombre de classes po  ssibles à prédire est très grand, nous avons analysé la possibilité de faire du clustering sur nos variables réponses pour créer des clusters de documents desquels on prédirait les 5 documents les plus fréquents. Ceci permettrait à notre modèle de travailler avec un nombre plus restreint de classes et ainsi de mieux capter le signal pour chacune d'elles.
Les clusters ainsi créés représentent donc des documents semblables.
Puisque le clustering des documents se fait sur leur titre, les clusters regroupent donc des documents traitant de sujets similaires.

Les différentes étapes de notre \emph{pipeline} sont donc les suivantes:
\begin{itemize}
	\item Fusion des données de recherche avec l'information des documents associés
	\item Filtre des champs conservés
	\item Normalisation des requêtes (stemming)
	\item Vectorisation des requêtes (tokenization, vecteur de fréquence, TF-IDF, Word2Vec)
	\item Transformation des variables catégoriques en variables indicatrices
	\item Imputation des données manquantes
\end{itemize}
	

