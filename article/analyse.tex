Les résultats montrent que le meilleur score pour le modèle MLP est plus de deux fois supérieur que le meilleur score pour le modèle KNN (0.360 vs 0.166). 
Ceci montre que la problématique de recommandation de documents est probablement trop complexe pour être modélisée par un algorithme basé sur le voisinage comme le KNN. 
Les deux méthodes fonctionnent bien avec un grand nombre de données et ont généralement un biais faible. 
Dans le cas du MLP, ce grand nombre de données a probablement permis l'entraînement d'un nombre important de neurones, ce qui est directement lié avec la capacité de notre classifieur à mieux modéliser le phénomène complexe de recommandation de documents. 
Par rapport à la problématique, le modèle KNN présente aussi un désavantage si on le compare au MLP en terme de temps d'évaluation lors de la présentation d'une nouvelle donnée. KNN doit stocker et utiliser les données pour déterminer le voisinage d'une nouvelle donnée en entrée. 
Cela peut représenter un obstable avec le nombre important de données utilisées pour bâtir le modèle, ce qui n'est pas souhaitable pour un engin de recherche qui doit idéalement donner une réponse rapidement pour satisfaire les exigences techniques d'un tel outil. 
En revanche, le MLP permet de ne conserver que les paramètres du modèle et permet ainsi un temps d'évaluation nettement réduit.
\break

Lorsque le \textit{clustering} de documents était utilisé pour se créer des étiquettes de données, les modèles étaient moins performants. 
Nous croyons qu'avec seulement le titre des documents, nous n'avions pas assez d'information pour faire des clusters significativement différents. 
De plus, une fois que le cluster était prédit, ce n'était peut-être pas optimal de simplement proposer les 5 documents les plus fréquents. 
Il est donc préférable de laisser les algorithmes apprendre directement les documents à prédire plutôt que des groupes de ceux-ci si on utilise une approche trop simpliste comme nous avons fait.
\break

Également, on remarque que, malgré la grande notoriété des plongements de mots en traitement de la langue naturelle, l'utilisation des \emph{embeddings} de mots ne donne pas de bons résultats dans notre contexte.
On explique cela par le fait que le vocabulaire de nos requêtes est assez particulier et contient des mots qui ne font pas partie de la langue usuelle.
Pour cette raison, le modèle de Google que nous avons utilisé ne permet pas bien de capter les ressemblances entre nos requêtes. 
Il aurait donc fallu procéder à un entraînement du modèle sur nos requêtes pour qu'il s'adapte mieux à notre contexte, mais puisque les plongements de mots sont basés sur l'entourage des mots et que nos requêtes sont très courtes, l'entraînement que nous avons n'a pas du tout amélioré les résultats.

\break

Maintenant, au-delà de la comparaison des deux algorithmes testés, il reste tout de même qu'un score final de 0.36 n'est pas une performance très bonne.
Plusieurs raisons peuvent être derrière ce faible taux de classement. 
D'une part, le contenu des documents était une donnée qu'on ne pouvait utiliser pour bâtir notre modèle puisque non présente.
Or, comme présenté au début de cet article, la littérature montre que c'est principalement sur cette information que se basent les techniques actuelles en recommandation de documents. 
L'extraction d'information pertinente pour la présente problématique a donc dû être faite principalement sur le texte des recherches. 
Ce type de texte est pour sa part beaucoup plus bruité (plus de mots inconnus/ne faisant pas partie de la langue usuelle) que le texte de documents ce qui rend plus difficile l'utilisation de modèles standards pré-entraînés.
